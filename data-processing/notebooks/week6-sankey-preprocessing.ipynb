{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 6: Sankey Visualization Data Preprocessing\n\n**Goal**: Process ACLED data to create actor-country-conflict type flows for Sankey visualization.\n\n**Output**: `viz10_actor_sankey.json` with:\n- Top 6 actors per actor type\n- Aggregated flows: Actor → Country → Event Type\n- Event counts and fatality statistics\n- Timeline data hooks for future implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore ACLED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ACLED data\n",
    "print(\"Loading ACLED dataset...\")\n",
    "df = pd.read_csv('../raw-data/ACLED/ACLED_2025-10-29.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {df['event_date'].min()} to {df['event_date'].max()}\")\n",
    "\n",
    "# Display basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key columns for Sankey visualization\n",
    "key_columns = ['actor1', 'inter1', 'country', 'event_type', 'sub_event_type', \n",
    "               'fatalities', 'year', 'event_date', 'event_id_cnty']\n",
    "\n",
    "print(\"Key columns for analysis:\")\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"✓ {col}: {df[col].dtype}, {df[col].isnull().sum()} nulls\")\n",
    "    else:\n",
    "        print(f\"✗ {col}: NOT FOUND\")\n",
    "\n",
    "# Sample data\n",
    "print(\"\\nSample data:\")\n",
    "df[key_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Actor Types and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor type distribution\n",
    "print(\"Actor Type Distribution (inter1):\")\n",
    "actor_type_counts = df['inter1'].value_counts()\n",
    "print(actor_type_counts)\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "actor_type_counts.plot(kind='bar')\n",
    "plt.title('Event Distribution by Actor Type')\n",
    "plt.xlabel('Actor Type')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event type distribution\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "event_type_counts = df['event_type'].value_counts()\n",
    "print(event_type_counts)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "event_type_counts.plot(kind='bar')\n",
    "plt.title('Distribution by Event Type')\n",
    "plt.xlabel('Event Type')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Identify Top 6 Actors per Actor Type"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count events per actor within each actor type\n",
    "actor_stats = df.groupby(['inter1', 'actor1']).agg({\n",
    "    'event_id_cnty': 'count',\n",
    "    'fatalities': 'sum',\n",
    "    'country': 'nunique'\n",
    "}).rename(columns={\n",
    "    'event_id_cnty': 'total_events',\n",
    "    'country': 'countries_active'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Total unique actors: {len(actor_stats)}\")\n",
    "print(\"\\nTop actors overall:\")\n",
    "print(actor_stats.nlargest(10, 'total_events')[['actor1', 'inter1', 'total_events', 'fatalities']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get top 6 actors per actor type\ntop_actors_by_type = {}\n\nfor actor_type in actor_stats['inter1'].unique():\n    if pd.isna(actor_type):\n        continue\n    \n    type_actors = actor_stats[actor_stats['inter1'] == actor_type].nlargest(6, 'total_events')\n    top_actors_by_type[actor_type] = type_actors['actor1'].tolist()\n    \n    print(f\"\\n{actor_type} - Top 6 Actors:\")\n    for i, (_, row) in enumerate(type_actors.iterrows(), 1):\n        print(f\"{i}. {row['actor1'][:60]:<60} | {row['total_events']:>6} events | {row['fatalities']:>7} fatalities\")\n\n# Summary\ntotal_selected_actors = sum(len(actors) for actors in top_actors_by_type.values())\nprint(f\"\\nTotal selected actors: {total_selected_actors}\")\nprint(f\"Actor types: {list(top_actors_by_type.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Data and Create Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all selected actors\n",
    "all_selected_actors = []\n",
    "for actor_list in top_actors_by_type.values():\n",
    "    all_selected_actors.extend(actor_list)\n",
    "\n",
    "print(f\"Filtering data for {len(all_selected_actors)} selected actors...\")\n",
    "\n",
    "# Filter dataset to only selected actors\n",
    "df_filtered = df[df['actor1'].isin(all_selected_actors)].copy()\n",
    "\n",
    "print(f\"Original dataset: {len(df):,} events\")\n",
    "print(f\"Filtered dataset: {len(df_filtered):,} events ({len(df_filtered)/len(df)*100:.1f}% of original)\")\n",
    "\n",
    "# Check data quality\n",
    "print(\"\\nData quality check:\")\n",
    "print(f\"Missing actor1: {df_filtered['actor1'].isnull().sum()}\")\n",
    "print(f\"Missing country: {df_filtered['country'].isnull().sum()}\")\n",
    "print(f\"Missing event_type: {df_filtered['event_type'].isnull().sum()}\")\n",
    "print(f\"Missing fatalities: {df_filtered['fatalities'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data - fill missing fatalities with 0\n",
    "df_filtered['fatalities'] = df_filtered['fatalities'].fillna(0)\n",
    "\n",
    "# Remove rows with missing essential data\n",
    "essential_columns = ['actor1', 'inter1', 'country', 'event_type']\n",
    "df_clean = df_filtered.dropna(subset=essential_columns)\n",
    "\n",
    "print(f\"After cleaning: {len(df_clean):,} events ({len(df_clean)/len(df_filtered)*100:.1f}% retained)\")\n",
    "\n",
    "# Convert year to int\n",
    "df_clean['year'] = df_clean['year'].astype(int)\n",
    "\n",
    "print(f\"Year range: {df_clean['year'].min()} - {df_clean['year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Flows by Actor-Country-Event Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flows: Actor → Country → Event Type\n",
    "print(\"Aggregating flows...\")\n",
    "\n",
    "flows = df_clean.groupby(['actor1', 'inter1', 'country', 'event_type']).agg({\n",
    "    'event_id_cnty': 'count',\n",
    "    'fatalities': 'sum', \n",
    "    'year': lambda x: sorted(x.unique()),\n",
    "    'sub_event_type': lambda x: x.mode().iloc[0] if not x.empty else 'Unknown'\n",
    "}).rename(columns={\n",
    "    'event_id_cnty': 'events'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Total flows created: {len(flows)}\")\n",
    "print(f\"Unique actors: {flows['actor1'].nunique()}\")\n",
    "print(f\"Unique countries: {flows['country'].nunique()}\")\n",
    "print(f\"Unique event types: {flows['event_type'].nunique()}\")\n",
    "\n",
    "# Display sample flows\n",
    "print(\"\\nSample flows:\")\n",
    "flows.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year-by-year breakdown for timeline hooks\n",
    "print(\"Creating year-by-year breakdown...\")\n",
    "\n",
    "year_breakdown = df_clean.groupby(['actor1', 'inter1', 'country', 'event_type', 'year']).agg({\n",
    "    'event_id_cnty': 'count',\n",
    "    'fatalities': 'sum'\n",
    "}).rename(columns={'event_id_cnty': 'events'}).reset_index()\n",
    "\n",
    "print(f\"Year-breakdown records: {len(year_breakdown)}\")\n",
    "print(\"Sample year breakdown:\")\n",
    "year_breakdown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Statistics for Color Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatality statistics for color scaling\n",
    "fatality_stats = {\n",
    "    'min': int(flows['fatalities'].min()),\n",
    "    'max': int(flows['fatalities'].max()),\n",
    "    'mean': float(flows['fatalities'].mean()),\n",
    "    'median': float(flows['fatalities'].median()),\n",
    "    'q25': float(flows['fatalities'].quantile(0.25)),\n",
    "    'q75': float(flows['fatalities'].quantile(0.75))\n",
    "}\n",
    "\n",
    "print(\"Fatality Statistics:\")\n",
    "for key, value in fatality_stats.items():\n",
    "    print(f\"{key}: {value:,.1f}\")\n",
    "\n",
    "# Event count statistics\n",
    "event_stats = {\n",
    "    'min': int(flows['events'].min()),\n",
    "    'max': int(flows['events'].max()),\n",
    "    'mean': float(flows['events'].mean()),\n",
    "    'median': float(flows['events'].median())\n",
    "}\n",
    "\n",
    "print(\"\\nEvent Count Statistics:\")\n",
    "for key, value in event_stats.items():\n",
    "    print(f\"{key}: {value:,.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Fatalities distribution (log scale)\n",
    "flows_nonzero_fatalities = flows[flows['fatalities'] > 0]\n",
    "ax1.hist(flows_nonzero_fatalities['fatalities'], bins=50, alpha=0.7)\n",
    "ax1.set_xlabel('Fatalities')\n",
    "ax1.set_ylabel('Number of Flows')\n",
    "ax1.set_title('Distribution of Fatalities (Non-zero only)')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Events distribution\n",
    "ax2.hist(flows['events'], bins=50, alpha=0.7)\n",
    "ax2.set_xlabel('Events')\n",
    "ax2.set_ylabel('Number of Flows')\n",
    "ax2.set_title('Distribution of Event Counts')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Flows with zero fatalities: {(flows['fatalities'] == 0).sum()} / {len(flows)} ({(flows['fatalities'] == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Final Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create year breakdown lookup dictionary (more robust approach)\n# Build a dictionary keyed by (actor, actor_type, country, event_type) -> year data\nprint(\"Building year breakdown lookup...\")\n\nyear_breakdown_lookup = {}\nfor _, row in year_breakdown.iterrows():\n    key = (row['actor1'], row['inter1'], row['country'], row['event_type'])\n    if key not in year_breakdown_lookup:\n        year_breakdown_lookup[key] = {}\n    year_breakdown_lookup[key][str(int(row['year']))] = {\n        'events': int(row['events']),\n        'fatalities': int(row['fatalities'])\n    }\n\nprint(f\"Created lookup with {len(year_breakdown_lookup)} unique flow keys\")\n\n# Test lookup\nsample_key = list(year_breakdown_lookup.keys())[0]\nprint(f\"\\nSample lookup for {sample_key}:\")\nprint(year_breakdown_lookup[sample_key])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create final dataset structure\nprint(\"Building final dataset structure...\")\n\n# Calculate statistics\nfatality_stats = {\n    'min': int(flows['fatalities'].min()),\n    'max': int(flows['fatalities'].max()),\n    'mean': float(flows['fatalities'].mean()),\n    'median': float(flows['fatalities'].median()),\n    'q25': float(flows['fatalities'].quantile(0.25)),\n    'q75': float(flows['fatalities'].quantile(0.75))\n}\n\nevent_stats = {\n    'min': int(flows['events'].min()),\n    'max': int(flows['events'].max()),\n    'mean': float(flows['events'].mean()),\n    'median': float(flows['events'].median())\n}\n\n# Convert flows to final format with year_breakdown from lookup\nflows_list = []\nmissing_year_breakdown = 0\n\nfor _, row in flows.iterrows():\n    # Get year breakdown from lookup dictionary\n    key = (row['actor1'], row['inter1'], row['country'], row['event_type'])\n    year_breakdown_dict = year_breakdown_lookup.get(key, {})\n    \n    if not year_breakdown_dict:\n        missing_year_breakdown += 1\n    \n    flow_dict = {\n        'actor': row['actor1'],\n        'actor_type': row['inter1'],\n        'country': row['country'],\n        'event_type': row['event_type'],\n        'sub_event_type': row['sub_event_type'],  # Added sub_event_type\n        'events': int(row['events']),\n        'fatalities': int(row['fatalities']),\n        'years': [int(y) for y in row['year']],\n        'year_breakdown': year_breakdown_dict\n    }\n    \n    flows_list.append(flow_dict)\n\nprint(f\"Created {len(flows_list)} flow records\")\nprint(f\"Flows missing year_breakdown: {missing_year_breakdown}\")\n\n# Verify year_breakdown is populated\npopulated = sum(1 for f in flows_list if f['year_breakdown'])\nprint(f\"Flows with populated year_breakdown: {populated}/{len(flows_list)}\")\n\n# Create final dataset\nfinal_dataset = {\n    'metadata': {\n        'created_date': datetime.now().isoformat(),\n        'source': 'ACLED_2025-10-29.csv',\n        'total_actors': len(all_selected_actors),\n        'total_flows': len(flows_list),\n        'year_range': [int(df_clean['year'].min()), int(df_clean['year'].max())],\n        'fatality_stats': fatality_stats,\n        'event_stats': event_stats\n    },\n    'actor_types': top_actors_by_type,\n    'flows': flows_list\n}\n\nprint(\"\\nFinal dataset structure:\")\nprint(f\"- Metadata: {len(final_dataset['metadata'])} fields\")\nprint(f\"- Actor types: {len(final_dataset['actor_types'])} categories\")\nprint(f\"- Flows: {len(final_dataset['flows'])} records\")\n\n# Show sample flow with year_breakdown\nprint(\"\\nSample flow with year_breakdown:\")\nsample_with_data = next((f for f in flows_list if f['year_breakdown']), None)\nif sample_with_data:\n    print(json.dumps(sample_with_data, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON file\n",
    "output_file = '../viz-datasets/viz10_actor_sankey.json'\n",
    "\n",
    "print(f\"Exporting to {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size = os.path.getsize(output_file) / 1024**2\n",
    "print(f\"File exported successfully!\")\n",
    "print(f\"File size: {file_size:.1f} MB\")\n",
    "\n",
    "# Test loading\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    test_load = json.load(f)\n",
    "\n",
    "print(f\"\\nVerification - loaded {len(test_load['flows'])} flows\")\n",
    "print(f\"Sample flow keys: {list(test_load['flows'][0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=== DATA PREPROCESSING COMPLETE ===\")\n",
    "print(f\"\\nOriginal dataset: {len(df):,} events\")\n",
    "print(f\"Processed flows: {len(flows_list):,}\")\n",
    "print(f\"Selected actors: {len(all_selected_actors)}\")\n",
    "print(f\"Actor types: {len(top_actors_by_type)}\")\n",
    "\n",
    "print(\"\\nActor type breakdown:\")\n",
    "for actor_type, actors in top_actors_by_type.items():\n",
    "    print(f\"- {actor_type}: {len(actors)} actors\")\n",
    "\n",
    "print(f\"\\nOutput file: {output_file}\")\n",
    "print(f\"File size: {file_size:.1f} MB\")\n",
    "\n",
    "print(\"\\n=== READY FOR PHASE 2: SANKEY IMPLEMENTATION ===\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Copy JSON to public/src/assets/data/\")\n",
    "print(\"2. Update SankeyChart.vue with actor dropdown\")\n",
    "print(\"3. Implement 3-level sankey visualization\")\n",
    "print(\"4. Add fatality-based color coding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}